<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>vincent blog (文章分类：scala)</title><link>https://wisonwang.github.io/</link><description></description><atom:link href="https://wisonwang.github.io/categories/scala.xml" rel="self" type="application/rss+xml"></atom:link><language>zh_cn</language><copyright>Contents © 2018 &lt;a href="mailto:fangfu2012@gmail.com"&gt;vincent wang&lt;/a&gt; </copyright><lastBuildDate>Wed, 18 Apr 2018 14:56:59 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>使用python还是scala开发spark程序？</title><link>https://wisonwang.github.io/posts/spark-with-python-or-scala/</link><dc:creator>vincent wang</dc:creator><description>&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;scala和python开发spark程序各自优点&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;/div&gt;&lt;div id="outline-container-sec-1-1" class="outline-3"&gt;
&lt;h3 id="sec-1-1"&gt;python优点&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-1-1"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;动态语言开发效率高，工程难度低
&lt;/li&gt;
&lt;li&gt;算法类库丰富，易于移移植现有算法到spark平台
&lt;/li&gt;
&lt;li&gt;工具支持丰富，包括jupyter notebook之类的工具，能够做到程序结果可视化，方便调优
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-1-2" class="outline-3"&gt;
&lt;h3 id="sec-1-2"&gt;python缺点&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-1-2"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;性能差： 动态语言载加上通过py4j来和spark交互的，在计算量很大的情况下慢的可怕；
&lt;/li&gt;
&lt;li&gt;因为不是通过py4j调用，不能像scala那样和java可以直接调用，导致不能直接调用java或者scala类库
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-sec-2" class="outline-2"&gt;
&lt;h2 id="sec-2"&gt;python 开发spark程序的一些问题&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;/div&gt;&lt;div id="outline-container-sec-2-1" class="outline-3"&gt;
&lt;h3 id="sec-2-1"&gt;python 依赖环境分发&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-2-1"&gt;
&lt;p&gt;
用 Java 和 Scala 访问 Spark 提供了许多优点 : 因为 Spark 它自己运行在 JVM 中，运行在 JVM 内部是平台无关的，独立的代码包和它打入到 JAR 文件中的依赖，以及更高的性能。如果您使用 Spark Python API 将会失去这些优势。
&lt;/p&gt;

&lt;p&gt;
管理依赖并让它们可用于群集上的 Python Job 是很难的。为了确定哪些依赖在群集上是需要的，您必须了解运行在分布式 群集 中的 Spark executor 进程中的 Spark 应用程序的代码。如果您定义的 Python 转换使用的任何的第三方库，比如 NumPy 或者 nltk，当它们运行在远程的 executor 上时 Spark executor 需要访问这些库。
&lt;/p&gt;

&lt;p&gt;
主要可以通过一下方式，推荐第四种方法：
&lt;/p&gt;

&lt;ol class="org-ol"&gt;
&lt;li&gt;独立的依赖关系
&lt;/li&gt;
&lt;li&gt;复杂的依赖关系
&lt;/li&gt;
&lt;li&gt;分发 Egg 文件的限制
&lt;/li&gt;
&lt;li&gt;设置 Python 路径(推荐)
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
&lt;a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=2886878"&gt;详细参考：运行 Spark Python 应用&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>pyspark</category><category>python</category><category>scala</category><category>spark</category><guid>https://wisonwang.github.io/posts/spark-with-python-or-scala/</guid><pubDate>Tue, 17 Apr 2018 08:16:02 GMT</pubDate></item></channel></rss>